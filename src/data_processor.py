import json
import os
from datetime import datetime
from typing import Dict, List
import logging
from pathlib import Path
import yaml

# åŠ¨æ€å¯¼å…¥aiofiles
try:
    import aiofiles
    AIOFILES_AVAILABLE = True
except ImportError:
    AIOFILES_AVAILABLE = False

class DataProcessor:
    """æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.base_data_dir = Path("data")
        
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        self._ensure_directories()
    
    def _ensure_directories(self):
        """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        directories = [
            self.base_data_dir / "papers",
            self.base_data_dir / "analysis", 
            self.base_data_dir / "statistics"
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
    
    async def _load_existing_papers(self, papers_dir: Path) -> List[Dict]:
        """åŠ è½½å·²å­˜åœ¨çš„è®ºæ–‡æ•°æ®"""
        papers_file = papers_dir / "papers.json"
        if not papers_file.exists():
            return []
        
        try:
            if AIOFILES_AVAILABLE:
                async with aiofiles.open(papers_file, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    return json.loads(content)
            else:
                with open(papers_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            self.logger.warning(f"è¯»å–å·²å­˜åœ¨è®ºæ–‡æ•°æ®å¤±è´¥: {e}")
            return []
    
    async def _load_existing_analysis(self, analysis_dir: Path) -> List[Dict]:
        """åŠ è½½å·²å­˜åœ¨çš„åˆ†æç»“æœ"""
        results_file = analysis_dir / "analysis_results.json"
        if not results_file.exists():
            return []
        
        try:
            if AIOFILES_AVAILABLE:
                async with aiofiles.open(results_file, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    return json.loads(content)
            else:
                with open(results_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            self.logger.warning(f"è¯»å–å·²å­˜åœ¨åˆ†æç»“æœå¤±è´¥: {e}")
            return []
    
    async def get_papers_needing_analysis(self, papers: List[Dict], date: str) -> List[Dict]:
        """è·å–éœ€è¦åˆ†æçš„è®ºæ–‡ï¼ˆæ–°è®ºæ–‡æˆ–åˆ†æå¤±è´¥çš„è®ºæ–‡ï¼‰"""
        analysis_dir = self.base_data_dir / "analysis" / date
        
        # åŠ è½½å·²å­˜åœ¨çš„åˆ†æç»“æœ
        existing_results = await self._load_existing_analysis(analysis_dir)
        
        # åˆ›å»ºå·²åˆ†æçš„è®ºæ–‡IDé›†åˆï¼ˆåªåŒ…å«æˆåŠŸåˆ†æçš„ï¼‰
        analyzed_ids = set()
        for result in existing_results:
            paper_id = result.get('paper_id')
            # åªæœ‰æˆåŠŸåˆ†æçš„è®ºæ–‡æ‰è¢«è®¤ä¸ºæ˜¯å·²åˆ†æçš„
            if paper_id and result.get('analysis') and not result.get('error'):
                analyzed_ids.add(paper_id)
        
        # ç­›é€‰éœ€è¦åˆ†æçš„è®ºæ–‡
        papers_to_analyze = []
        for paper in papers:
            arxiv_id = paper.get('arxiv_id')
            if arxiv_id:
                if arxiv_id not in analyzed_ids:
                    papers_to_analyze.append(paper)
                    if arxiv_id in {r.get('paper_id') for r in existing_results}:
                        self.logger.info(f"ğŸ”„ é‡æ–°åˆ†æå¤±è´¥çš„è®ºæ–‡: {arxiv_id}")
                    else:
                        self.logger.info(f"ğŸ†• æ–°è®ºæ–‡å¾…åˆ†æ: {arxiv_id}")
                else:
                    self.logger.info(f"âœ… è·³è¿‡å·²åˆ†æè®ºæ–‡: {arxiv_id}")
        
        return papers_to_analyze
    
    async def save_papers(self, papers: List[Dict], date: str):
        """ä¿å­˜åŸå§‹è®ºæ–‡æ•°æ®ï¼ˆå»é‡å¤„ç†ï¼‰"""
        papers_dir = self.base_data_dir / "papers" / date
        papers_dir.mkdir(parents=True, exist_ok=True)
        
        # è¯»å–å·²å­˜åœ¨çš„è®ºæ–‡æ•°æ®
        existing_papers = await self._load_existing_papers(papers_dir)
        existing_ids = {paper.get('arxiv_id') for paper in existing_papers}
        
        # åˆå¹¶æ–°è®ºæ–‡å’Œå·²å­˜åœ¨çš„è®ºæ–‡ï¼Œå»é‡å¹¶æ›´æ–°
        merged_papers = {}
        
        # å…ˆåŠ è½½å·²å­˜åœ¨çš„è®ºæ–‡
        for paper in existing_papers:
            arxiv_id = paper.get('arxiv_id')
            if arxiv_id:
                merged_papers[arxiv_id] = paper
        
        # æ·»åŠ æˆ–æ›´æ–°æ–°è®ºæ–‡
        new_count = 0
        updated_count = 0
        for paper in papers:
            arxiv_id = paper.get('arxiv_id')
            if arxiv_id:
                if arxiv_id in existing_ids:
                    self.logger.info(f"ğŸ“ æ›´æ–°è®ºæ–‡: {arxiv_id}")
                    updated_count += 1
                else:
                    self.logger.info(f"ğŸ†• æ–°å¢è®ºæ–‡: {arxiv_id}")
                    new_count += 1
                merged_papers[arxiv_id] = paper
        
        # è½¬æ¢ä¸ºåˆ—è¡¨
        final_papers = list(merged_papers.values())
        
        # ä¿å­˜æ‰€æœ‰è®ºæ–‡åˆ°ä¸€ä¸ªJSONæ–‡ä»¶
        papers_file = papers_dir / "papers.json"
        if AIOFILES_AVAILABLE:
            async with aiofiles.open(papers_file, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(final_papers, ensure_ascii=False, indent=2))
        else:
            with open(papers_file, 'w', encoding='utf-8') as f:
                f.write(json.dumps(final_papers, ensure_ascii=False, indent=2))
        
        # ä¸ºæ¯ç¯‡è®ºæ–‡åˆ›å»ºå•ç‹¬çš„æ–‡ä»¶
        for paper in final_papers:
            arxiv_id = paper.get('arxiv_id', 'unknown')
            paper_file = papers_dir / f"{arxiv_id}.json"
            if AIOFILES_AVAILABLE:
                async with aiofiles.open(paper_file, 'w', encoding='utf-8') as f:
                    await f.write(json.dumps(paper, ensure_ascii=False, indent=2))
            else:
                with open(paper_file, 'w', encoding='utf-8') as f:
                    f.write(json.dumps(paper, ensure_ascii=False, indent=2))
        
        self.logger.info(f"âœ… è®ºæ–‡æ•°æ®å¤„ç†å®Œæˆ: æ€»è®¡ {len(final_papers)} ç¯‡ (æ–°å¢ {new_count}, æ›´æ–° {updated_count})")
    
    async def save_analysis_results(self, analysis_results: List[Dict], date: str):
        """ä¿å­˜åˆ†æç»“æœï¼ˆå»é‡å¤„ç†ï¼‰"""
        analysis_dir = self.base_data_dir / "analysis" / date
        analysis_dir.mkdir(parents=True, exist_ok=True)
        
        # è¯»å–å·²å­˜åœ¨çš„åˆ†æç»“æœ
        existing_results = await self._load_existing_analysis(analysis_dir)
        existing_ids = {result.get('paper_id') for result in existing_results}
        
        # åˆå¹¶æ–°åˆ†æç»“æœå’Œå·²å­˜åœ¨çš„ç»“æœï¼Œå»é‡å¹¶æ›´æ–°
        merged_results = {}
        
        # å…ˆåŠ è½½å·²å­˜åœ¨çš„åˆ†æç»“æœ
        for result in existing_results:
            paper_id = result.get('paper_id')
            if paper_id:
                merged_results[paper_id] = result
        
        # æ·»åŠ æˆ–æ›´æ–°æ–°åˆ†æç»“æœ
        new_count = 0
        updated_count = 0
        for result in analysis_results:
            paper_id = result.get('paper_id')
            if paper_id:
                if paper_id in existing_ids:
                    self.logger.info(f"ğŸ“ æ›´æ–°åˆ†æç»“æœ: {paper_id}")
                    updated_count += 1
                else:
                    self.logger.info(f"ğŸ†• æ–°å¢åˆ†æç»“æœ: {paper_id}")
                    new_count += 1
                # æ›´æ–°æ—¶é—´æˆ³
                result['timestamp'] = datetime.now().isoformat()
                merged_results[paper_id] = result
        
        # è½¬æ¢ä¸ºåˆ—è¡¨
        final_results = list(merged_results.values())
        
        # ä¿å­˜æ‰€æœ‰åˆ†æç»“æœ
        results_file = analysis_dir / "analysis_results.json"
        if AIOFILES_AVAILABLE:
            async with aiofiles.open(results_file, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(final_results, ensure_ascii=False, indent=2))
        else:
            with open(results_file, 'w', encoding='utf-8') as f:
                f.write(json.dumps(final_results, ensure_ascii=False, indent=2))
        
        # ä¸ºæ¯ä¸ªåˆ†æç»“æœåˆ›å»ºå•ç‹¬çš„æ–‡ä»¶
        for result in final_results:
            paper_id = result.get('paper_id', 'unknown')
            result_file = analysis_dir / f"{paper_id}_analysis.json"
            if AIOFILES_AVAILABLE:
                async with aiofiles.open(result_file, 'w', encoding='utf-8') as f:
                    await f.write(json.dumps(result, ensure_ascii=False, indent=2))
            else:
                with open(result_file, 'w', encoding='utf-8') as f:
                    f.write(json.dumps(result, ensure_ascii=False, indent=2))
        
        self.logger.info(f"âœ… åˆ†æç»“æœå¤„ç†å®Œæˆ: æ€»è®¡ {len(final_results)} ä¸ª (æ–°å¢ {new_count}, æ›´æ–° {updated_count})")
    
    async def generate_daily_summary(self, analysis_results: List[Dict], date: str):
        """ç”Ÿæˆæ¯æ—¥æ€»ç»“æŠ¥å‘Š"""
        analysis_dir = self.base_data_dir / "analysis" / date
        papers_dir = self.base_data_dir / "papers" / date
        
        # åŠ è½½è®ºæ–‡æ•°æ®
        papers_data = await self._load_existing_papers(papers_dir)
        papers_dict = {paper.get('arxiv_id'): paper for paper in papers_data}
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_papers = len(analysis_results)
        successful_analyses = sum(1 for r in analysis_results if r.get('analysis') and not r.get('error'))
        
        # åˆ†ç±»ç»Ÿè®¡
        category_stats = {}
        for result in analysis_results:
            classification = result.get('classification', {})
            category = classification.get('category', 'æœªåˆ†ç±»') if classification else 'æœªåˆ†ç±»'
            category_stats[category] = category_stats.get(category, 0) + 1
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        summary_content = f"""# ArXivåŠ é€Ÿå™¨ç‰©ç†è®ºæ–‡æ¯æ—¥åˆ†ææŠ¥å‘Š

**æ—¥æœŸ**: {date}
**åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š æ¦‚è§ˆç»Ÿè®¡

- **æ€»è®ºæ–‡æ•°**: {total_papers}
- **æˆåŠŸåˆ†æ**: {successful_analyses}
- **åˆ†ææˆåŠŸç‡**: {successful_analyses/total_papers*100:.1f}% (å¦‚æœä¸º0ï¼Œè¯´æ˜éœ€è¦è®¾ç½®APIå¯†é’¥)

## ğŸ“ˆ åˆ†ç±»åˆ†å¸ƒ

"""
        
        for category, count in sorted(category_stats.items()):
            percentage = count / total_papers * 100
            summary_content += f"- **{category}**: {count} ç¯‡ ({percentage:.1f}%)\n"
        
        summary_content += "\n## ğŸ“š è®ºæ–‡è¯¦æƒ…\n\n"
        
        # æ·»åŠ æ¯ç¯‡è®ºæ–‡çš„ç®€è¦ä¿¡æ¯
        for i, result in enumerate(analysis_results, 1):
            paper_id = result.get('paper_id')
            paper = papers_dict.get(paper_id, {})
            
            analysis = result.get('analysis', '')
            classification = result.get('classification', {})
            summary = result.get('summary', '')
            error = result.get('error')
            
            title = paper.get('title', 'æœªçŸ¥æ ‡é¢˜')
            authors = ', '.join(paper.get('authors', [])[:3])  # åªæ˜¾ç¤ºå‰3ä¸ªä½œè€…
            if len(paper.get('authors', [])) > 3:
                authors += " ç­‰"
            
            category = classification.get('category', 'æœªåˆ†ç±»') if classification else 'æœªåˆ†ç±»'
            arxiv_id = paper.get('arxiv_id', paper_id)
            
            summary_content += f"""### {i}. {title}

**ä½œè€…**: {authors}  
**ArXiv ID**: [{arxiv_id}](https://arxiv.org/abs/{arxiv_id})  
**åˆ†ç±»**: {category}  

"""
            
            if error:
                summary_content += f"**çŠ¶æ€**: âŒ åˆ†æå¤±è´¥ - {error}\n\n"
            elif summary:
                summary_content += f"**ç®€è¦æ€»ç»“**: {summary[:200]}{'...' if len(summary) > 200 else ''}\n\n"
            else:
                summary_content += "**çŠ¶æ€**: â³ ç­‰å¾…LLMåˆ†æï¼ˆéœ€è¦è®¾ç½®APIå¯†é’¥ï¼‰\n\n"
            
            summary_content += "---\n\n"
        
        # ä¿å­˜æ€»ç»“æŠ¥å‘Š
        summary_file = analysis_dir / "daily_summary.md"
        if AIOFILES_AVAILABLE:
            async with aiofiles.open(summary_file, 'w', encoding='utf-8') as f:
                await f.write(summary_content)
        else:
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(summary_content)
        
        self.logger.info(f"âœ… å·²ç”Ÿæˆæ¯æ—¥æ€»ç»“æŠ¥å‘Š: {summary_file}")
    
    async def update_statistics(self, analysis_results: List[Dict], date: str):
        """æ›´æ–°æ€»ä½“ç»Ÿè®¡ä¿¡æ¯"""
        stats_file = self.base_data_dir / "statistics" / "overall_stats.json"
        
        # åŠ è½½ç°æœ‰ç»Ÿè®¡
        overall_stats = {}
        if stats_file.exists():
            try:
                async with aiofiles.open(stats_file, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    overall_stats = json.loads(content)
            except Exception as e:
                self.logger.warning(f"åŠ è½½ç»Ÿè®¡æ–‡ä»¶å¤±è´¥: {e}")
        
        # åˆå§‹åŒ–ç»Ÿè®¡ç»“æ„
        if 'daily_counts' not in overall_stats:
            overall_stats['daily_counts'] = {}
        if 'category_totals' not in overall_stats:
            overall_stats['category_totals'] = {}
        if 'total_papers' not in overall_stats:
            overall_stats['total_papers'] = 0
        if 'last_updated' not in overall_stats:
            overall_stats['last_updated'] = date
        
        # æ›´æ–°ç»Ÿè®¡
        overall_stats['daily_counts'][date] = len(analysis_results)
        overall_stats['total_papers'] += len(analysis_results)
        overall_stats['last_updated'] = date
        
        # æ›´æ–°åˆ†ç±»ç»Ÿè®¡
        for result in analysis_results:
            classification = result.get('classification', {})
            category = classification.get('category_name', 'æœªåˆ†ç±»')
            overall_stats['category_totals'][category] = overall_stats['category_totals'].get(category, 0) + 1
        
        # ä¿å­˜æ›´æ–°åçš„ç»Ÿè®¡
        async with aiofiles.open(stats_file, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(overall_stats, ensure_ascii=False, indent=2))
        
        # ç”Ÿæˆç»Ÿè®¡å›¾è¡¨æ•°æ®
        await self._generate_stats_charts(overall_stats)
        
        self.logger.info(f"âœ… å·²æ›´æ–°æ€»ä½“ç»Ÿè®¡ä¿¡æ¯")
    
    async def _generate_stats_charts(self, overall_stats: Dict):
        """ç”Ÿæˆç»Ÿè®¡å›¾è¡¨æ•°æ®"""
        stats_dir = self.base_data_dir / "statistics"
        
        # ç”Ÿæˆæ¯æ—¥è®ºæ–‡æ•°é‡è¶‹åŠ¿æ•°æ®
        daily_trend = {
            "labels": list(overall_stats['daily_counts'].keys()),
            "data": list(overall_stats['daily_counts'].values())
        }
        
        trend_file = stats_dir / "daily_trend.json"
        async with aiofiles.open(trend_file, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(daily_trend, ensure_ascii=False, indent=2))
        
        # ç”Ÿæˆåˆ†ç±»åˆ†å¸ƒæ•°æ®
        category_distribution = {
            "labels": list(overall_stats['category_totals'].keys()),
            "data": list(overall_stats['category_totals'].values())
        }
        
        category_file = stats_dir / "category_distribution.json"
        async with aiofiles.open(category_file, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(category_distribution, ensure_ascii=False, indent=2))
        
        # ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
        stats_summary = {
            "total_papers": overall_stats['total_papers'],
            "total_days": len(overall_stats['daily_counts']),
            "average_papers_per_day": overall_stats['total_papers'] / max(len(overall_stats['daily_counts']), 1),
            "most_common_category": max(overall_stats['category_totals'].items(), key=lambda x: x[1]) if overall_stats['category_totals'] else ("æ— ", 0),
            "last_updated": overall_stats['last_updated']
        }
        
        summary_file = stats_dir / "summary.json"
        async with aiofiles.open(summary_file, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(stats_summary, ensure_ascii=False, indent=2))
