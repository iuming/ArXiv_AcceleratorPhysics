import json
import os
from datetime import datetime
from typing import Dict, List
import logging
from pathlib import Path
import yaml

# åŠ¨æ€å¯¼å…¥aiofiles
try:
    import aiofiles
    AIOFILES_AVAILABLE = True
except ImportError:
    AIOFILES_AVAILABLE = False

class DataProcessor:
    """æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.base_data_dir = Path("data")
        
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        self._ensure_directories()
    
    def _ensure_directories(self):
        """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        directories = [
            self.base_data_dir / "papers",
            self.base_data_dir / "analysis", 
            self.base_data_dir / "statistics"
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
    
    async def save_papers(self, papers: List[Dict], date: str):
        """ä¿å­˜åŸå§‹è®ºæ–‡æ•°æ®"""
        papers_dir = self.base_data_dir / "papers" / date
        papers_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ‰€æœ‰è®ºæ–‡åˆ°ä¸€ä¸ªJSONæ–‡ä»¶
        papers_file = papers_dir / "papers.json"
        if AIOFILES_AVAILABLE:
            async with aiofiles.open(papers_file, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(papers, ensure_ascii=False, indent=2))
        else:
            with open(papers_file, 'w', encoding='utf-8') as f:
                f.write(json.dumps(papers, ensure_ascii=False, indent=2))
        
        # ä¸ºæ¯ç¯‡è®ºæ–‡åˆ›å»ºå•ç‹¬çš„æ–‡ä»¶
        for paper in papers:
            arxiv_id = paper.get('arxiv_id', 'unknown')
            paper_file = papers_dir / f"{arxiv_id}.json"
            if AIOFILES_AVAILABLE:
                async with aiofiles.open(paper_file, 'w', encoding='utf-8') as f:
                    await f.write(json.dumps(paper, ensure_ascii=False, indent=2))
            else:
                with open(paper_file, 'w', encoding='utf-8') as f:
                    f.write(json.dumps(paper, ensure_ascii=False, indent=2))
        
        self.logger.info(f"âœ… å·²ä¿å­˜ {len(papers)} ç¯‡è®ºæ–‡æ•°æ®åˆ° {papers_dir}")
    
    async def save_analysis_results(self, analysis_results: List[Dict], date: str):
        """ä¿å­˜åˆ†æç»“æœ"""
        analysis_dir = self.base_data_dir / "analysis" / date
        analysis_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ‰€æœ‰åˆ†æç»“æœ
        results_file = analysis_dir / "analysis_results.json"
        if AIOFILES_AVAILABLE:
            async with aiofiles.open(results_file, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(analysis_results, ensure_ascii=False, indent=2))
        else:
            with open(results_file, 'w', encoding='utf-8') as f:
                f.write(json.dumps(analysis_results, ensure_ascii=False, indent=2))
        
        # ä¸ºæ¯ä¸ªåˆ†æç»“æœåˆ›å»ºå•ç‹¬çš„æ–‡ä»¶
        for result in analysis_results:
            paper_id = result.get('paper_id', 'unknown')
            result_file = analysis_dir / f"{paper_id}_analysis.json"
            if AIOFILES_AVAILABLE:
                async with aiofiles.open(result_file, 'w', encoding='utf-8') as f:
                    await f.write(json.dumps(result, ensure_ascii=False, indent=2))
            else:
                with open(result_file, 'w', encoding='utf-8') as f:
                    f.write(json.dumps(result, ensure_ascii=False, indent=2))
        
        self.logger.info(f"âœ… å·²ä¿å­˜ {len(analysis_results)} ä¸ªåˆ†æç»“æœåˆ° {analysis_dir}")
    
    async def generate_daily_summary(self, analysis_results: List[Dict], date: str):
        """ç”Ÿæˆæ¯æ—¥æ€»ç»“æŠ¥å‘Š"""
        analysis_dir = self.base_data_dir / "analysis" / date
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_papers = len(analysis_results)
        successful_analyses = sum(1 for r in analysis_results if r.get('analysis'))
        
        # åˆ†ç±»ç»Ÿè®¡
        category_stats = {}
        for result in analysis_results:
            classification = result.get('classification', {})
            category = classification.get('category_name', 'æœªåˆ†ç±»')
            category_stats[category] = category_stats.get(category, 0) + 1
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        summary_content = f"""# ArXivåŠ é€Ÿå™¨ç‰©ç†è®ºæ–‡æ¯æ—¥åˆ†ææŠ¥å‘Š

**æ—¥æœŸ**: {date}
**åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š æ¦‚è§ˆç»Ÿè®¡

- **æ€»è®ºæ–‡æ•°**: {total_papers}
- **æˆåŠŸåˆ†æ**: {successful_analyses}
- **åˆ†ææˆåŠŸç‡**: {successful_analyses/total_papers*100:.1f}%

## ğŸ“ˆ åˆ†ç±»åˆ†å¸ƒ

"""
        
        for category, count in sorted(category_stats.items()):
            percentage = count / total_papers * 100
            summary_content += f"- **{category}**: {count} ç¯‡ ({percentage:.1f}%)\n"
        
        summary_content += "\n## ğŸ“š è®ºæ–‡è¯¦æƒ…\n\n"
        
        # æ·»åŠ æ¯ç¯‡è®ºæ–‡çš„ç®€è¦ä¿¡æ¯
        for i, result in enumerate(analysis_results, 1):
            paper = result.get('paper', {})
            analysis = result.get('analysis', '')
            classification = result.get('classification', {})
            summary = result.get('summary', '')
            
            title = paper.get('title', 'æœªçŸ¥æ ‡é¢˜')
            authors = ', '.join(paper.get('authors', [])[:3])  # åªæ˜¾ç¤ºå‰3ä¸ªä½œè€…
            if len(paper.get('authors', [])) > 3:
                authors += " ç­‰"
            
            category = classification.get('category_name', 'æœªåˆ†ç±»')
            arxiv_id = paper.get('arxiv_id', '')
            
            summary_content += f"""### {i}. {title}

**ä½œè€…**: {authors}  
**ArXiv ID**: [{arxiv_id}](https://arxiv.org/abs/{arxiv_id})  
**åˆ†ç±»**: {category}  

**ç®€è¦æ€»ç»“**: {summary[:200]}{'...' if len(summary) > 200 else ''}

---

"""
        
        # ä¿å­˜æ€»ç»“æŠ¥å‘Š
        summary_file = analysis_dir / "daily_summary.md"
        async with aiofiles.open(summary_file, 'w', encoding='utf-8') as f:
            await f.write(summary_content)
        
        self.logger.info(f"âœ… å·²ç”Ÿæˆæ¯æ—¥æ€»ç»“æŠ¥å‘Š: {summary_file}")
    
    async def update_statistics(self, analysis_results: List[Dict], date: str):
        """æ›´æ–°æ€»ä½“ç»Ÿè®¡ä¿¡æ¯"""
        stats_file = self.base_data_dir / "statistics" / "overall_stats.json"
        
        # åŠ è½½ç°æœ‰ç»Ÿè®¡
        overall_stats = {}
        if stats_file.exists():
            try:
                async with aiofiles.open(stats_file, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    overall_stats = json.loads(content)
            except Exception as e:
                self.logger.warning(f"åŠ è½½ç»Ÿè®¡æ–‡ä»¶å¤±è´¥: {e}")
        
        # åˆå§‹åŒ–ç»Ÿè®¡ç»“æ„
        if 'daily_counts' not in overall_stats:
            overall_stats['daily_counts'] = {}
        if 'category_totals' not in overall_stats:
            overall_stats['category_totals'] = {}
        if 'total_papers' not in overall_stats:
            overall_stats['total_papers'] = 0
        if 'last_updated' not in overall_stats:
            overall_stats['last_updated'] = date
        
        # æ›´æ–°ç»Ÿè®¡
        overall_stats['daily_counts'][date] = len(analysis_results)
        overall_stats['total_papers'] += len(analysis_results)
        overall_stats['last_updated'] = date
        
        # æ›´æ–°åˆ†ç±»ç»Ÿè®¡
        for result in analysis_results:
            classification = result.get('classification', {})
            category = classification.get('category_name', 'æœªåˆ†ç±»')
            overall_stats['category_totals'][category] = overall_stats['category_totals'].get(category, 0) + 1
        
        # ä¿å­˜æ›´æ–°åçš„ç»Ÿè®¡
        async with aiofiles.open(stats_file, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(overall_stats, ensure_ascii=False, indent=2))
        
        # ç”Ÿæˆç»Ÿè®¡å›¾è¡¨æ•°æ®
        await self._generate_stats_charts(overall_stats)
        
        self.logger.info(f"âœ… å·²æ›´æ–°æ€»ä½“ç»Ÿè®¡ä¿¡æ¯")
    
    async def _generate_stats_charts(self, overall_stats: Dict):
        """ç”Ÿæˆç»Ÿè®¡å›¾è¡¨æ•°æ®"""
        stats_dir = self.base_data_dir / "statistics"
        
        # ç”Ÿæˆæ¯æ—¥è®ºæ–‡æ•°é‡è¶‹åŠ¿æ•°æ®
        daily_trend = {
            "labels": list(overall_stats['daily_counts'].keys()),
            "data": list(overall_stats['daily_counts'].values())
        }
        
        trend_file = stats_dir / "daily_trend.json"
        async with aiofiles.open(trend_file, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(daily_trend, ensure_ascii=False, indent=2))
        
        # ç”Ÿæˆåˆ†ç±»åˆ†å¸ƒæ•°æ®
        category_distribution = {
            "labels": list(overall_stats['category_totals'].keys()),
            "data": list(overall_stats['category_totals'].values())
        }
        
        category_file = stats_dir / "category_distribution.json"
        async with aiofiles.open(category_file, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(category_distribution, ensure_ascii=False, indent=2))
        
        # ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
        stats_summary = {
            "total_papers": overall_stats['total_papers'],
            "total_days": len(overall_stats['daily_counts']),
            "average_papers_per_day": overall_stats['total_papers'] / max(len(overall_stats['daily_counts']), 1),
            "most_common_category": max(overall_stats['category_totals'].items(), key=lambda x: x[1]) if overall_stats['category_totals'] else ("æ— ", 0),
            "last_updated": overall_stats['last_updated']
        }
        
        summary_file = stats_dir / "summary.json"
        async with aiofiles.open(summary_file, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(stats_summary, ensure_ascii=False, indent=2))
