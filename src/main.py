import asyncio
import os
import sys
from datetime import datetime
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(current_dir))

# åˆ‡æ¢åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼ˆä¸ºäº†æ­£ç¡®è®¿é—®é…ç½®æ–‡ä»¶ï¼‰
os.chdir(project_root)

# ç›´æ¥å¯¼å…¥æ¨¡å—ï¼ˆä¸ä½¿ç”¨srcå‰ç¼€ï¼‰
from arxiv_fetcher import ArXivFetcher
from llm_analyzer import LLMAnalyzer
from data_processor import DataProcessor
from utils import setup_logging, load_config

async def main():
    """ä¸»å‡½æ•°ï¼šåè°ƒæ•´ä¸ªåˆ†ææµç¨‹"""
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging()
    logger.info("å¼€å§‹æ¯æ—¥ArXivåŠ é€Ÿå™¨ç‰©ç†è®ºæ–‡åˆ†æ")
    
    # æ£€æŸ¥APIå¯†é’¥
    logger.info("ğŸ”‘ æ£€æŸ¥APIå¯†é’¥é…ç½®...")
    openai_key = os.getenv('OPENAI_API_KEY')
    deepseek_key = os.getenv('DEEPSEEK_API_KEY')
    hepai_key = os.getenv('HAI_API_KEY')
    anthropic_key = os.getenv('ANTHROPIC_API_KEY')
    
    available_apis = []
    if deepseek_key:
        available_apis.append("DeepSeek (æ¨è)")
    if hepai_key:
        available_apis.append("HEPAI (å¤‡ç”¨)")
    if openai_key:
        available_apis.append("OpenAI")
    if anthropic_key:
        available_apis.append("Anthropic")
    
    if not available_apis:
        logger.error("âŒ æœªè®¾ç½®ä»»ä½•LLM APIå¯†é’¥ï¼")
        logger.error("è¯·åœ¨GitHubä»“åº“Settings -> Secretsä¸­è®¾ç½®ï¼š")
        logger.error("- DEEPSEEK_API_KEY (æ¨èï¼Œæ€§ä»·æ¯”é«˜)")
        logger.error("- HAI_API_KEY (HEPAIï¼Œä¸­ç§‘é™¢é«˜èƒ½æ‰€)")
        logger.error("- OPENAI_API_KEY (å¤‡ç”¨)")
        logger.error("- ANTHROPIC_API_KEY (å¤‡ç”¨)")
        logger.error("ç³»ç»Ÿå°†ä»…æŠ“å–è®ºæ–‡ï¼Œä¸è¿›è¡ŒLLMåˆ†æ")
    else:
        logger.info(f"âœ… å·²è®¾ç½®APIå¯†é’¥: {', '.join(available_apis)}")
    
    # åŠ è½½é…ç½®
    config = load_config()
    
    # è·å–å½“å‰æ—¥æœŸ
    today = datetime.now().strftime("%Y-%m-%d")
    logger.info(f"åˆ†ææ—¥æœŸ: {today}")
    
    try:
        # åˆå§‹åŒ–ç»„ä»¶
        fetcher = ArXivFetcher(config)
        analyzer = LLMAnalyzer(config)
        processor = DataProcessor(config)
        
        # 1. æŠ“å–æœ€æ–°è®ºæ–‡
        logger.info("æ­£åœ¨æŠ“å–ArXivæœ€æ–°è®ºæ–‡...")
        papers = await fetcher.fetch_recent_papers()
        logger.info(f"æˆåŠŸæŠ“å– {len(papers)} ç¯‡è®ºæ–‡")
        
        if not papers:
            logger.warning("ä»Šæ—¥æ— æ–°è®ºæ–‡ï¼Œè·³è¿‡åˆ†æ")
            return
        
        # 2. ä¿å­˜åŸå§‹è®ºæ–‡æ•°æ®
        await processor.save_papers(papers, today)
        
        # 3. ä½¿ç”¨LLMåˆ†æè®ºæ–‡
        logger.info("å¼€å§‹LLMåˆ†æ...")
        analysis_results = []
        
        for i, paper in enumerate(papers, 1):
            logger.info(f"åˆ†æè®ºæ–‡ {i}/{len(papers)}: {paper['title'][:50]}...")
            
            try:
                analysis = await analyzer.analyze_paper(paper)
                analysis_results.append({
                    'paper': paper,
                    'analysis': analysis
                })
            except Exception as e:
                logger.error(f"åˆ†æè®ºæ–‡å¤±è´¥: {e}")
                continue
        
        # 4. ä¿å­˜åˆ†æç»“æœ
        await processor.save_analysis_results(analysis_results, today)
        
        # 5. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        await processor.generate_daily_summary(analysis_results, today)
        
        # 6. æ›´æ–°æ€»ä½“ç»Ÿè®¡
        await processor.update_statistics(analysis_results, today)
        
        logger.info(f"âœ… åˆ†æå®Œæˆï¼å¤„ç†äº† {len(analysis_results)} ç¯‡è®ºæ–‡")
        
    except Exception as e:
        logger.error(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main())
