#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Project: ArXiv_AcceleratorPhysics
File: main.py
Author: Ming Liu
Email: ming-1018@foxm        logger.error(f"ä¸¥é‡é”™è¯¯: {e}")
        raise

def cli_main():
    """å‘½ä»¤è¡Œå…¥å£ç‚¹å‡½æ•°"""
    asyncio.run(main())

if __name__ == "__main__":
    cli_main()om
Institution: Institute of High Energy Physics, Chinese Academy of Sciences
Created: July 25th, 2025
Description: ä¸»ç¨‹åºå…¥å£ï¼Œåè°ƒæ•´ä¸ªArXivåŠ é€Ÿå™¨ç‰©ç†è®ºæ–‡è‡ªåŠ¨åˆ†ææµç¨‹
             åŒ…æ‹¬è®ºæ–‡æŠ“å–ã€LLMåˆ†æã€æ•°æ®å¤„ç†å’Œç»Ÿè®¡æŠ¥å‘Šç”Ÿæˆ

Modification History:
- 2025-07-25: Initial creation
- 2025-07-25: Added comprehensive logging and error handling
"""

import asyncio
import os
import sys
from datetime import datetime
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(current_dir))

# åˆ‡æ¢åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼ˆä¸ºäº†æ­£ç¡®è®¿é—®é…ç½®æ–‡ä»¶ï¼‰
os.chdir(project_root)

# ç›´æ¥å¯¼å…¥æ¨¡å—ï¼ˆä¸ä½¿ç”¨srcå‰ç¼€ï¼‰
from arxiv_fetcher import ArXivFetcher
from llm_analyzer import LLMAnalyzer
from data_processor import DataProcessor
from utils import setup_logging, load_config

async def main():
    """ä¸»å‡½æ•°ï¼šåè°ƒæ•´ä¸ªåˆ†ææµç¨‹"""
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging()
    logger.info("å¼€å§‹æ¯æ—¥ArXivåŠ é€Ÿå™¨ç‰©ç†è®ºæ–‡åˆ†æ")
    
    # æ£€æŸ¥APIå¯†é’¥
    logger.info("ğŸ”‘ æ£€æŸ¥APIå¯†é’¥é…ç½®...")
    openai_key = os.getenv('OPENAI_API_KEY')
    deepseek_key = os.getenv('DEEPSEEK_API_KEY')
    hepai_key = os.getenv('HAI_API_KEY')
    anthropic_key = os.getenv('ANTHROPIC_API_KEY')
    
    available_apis = []
    if deepseek_key:
        available_apis.append("DeepSeek (æ¨è)")
    if hepai_key:
        available_apis.append("HEPAI (å¤‡ç”¨)")
    if openai_key:
        available_apis.append("OpenAI")
    if anthropic_key:
        available_apis.append("Anthropic")
    
    if not available_apis:
        logger.error("âŒ æœªè®¾ç½®ä»»ä½•LLM APIå¯†é’¥ï¼")
        logger.error("è¯·åœ¨GitHubä»“åº“Settings -> Secretsä¸­è®¾ç½®ï¼š")
        logger.error("- DEEPSEEK_API_KEY (æ¨èï¼Œæ€§ä»·æ¯”é«˜)")
        logger.error("- HAI_API_KEY (HEPAIï¼Œä¸­ç§‘é™¢é«˜èƒ½æ‰€)")
        logger.error("- OPENAI_API_KEY (å¤‡ç”¨)")
        logger.error("- ANTHROPIC_API_KEY (å¤‡ç”¨)")
        logger.error("ç³»ç»Ÿå°†ä»…æŠ“å–è®ºæ–‡ï¼Œä¸è¿›è¡ŒLLMåˆ†æ")
    else:
        logger.info(f"âœ… å·²è®¾ç½®APIå¯†é’¥: {', '.join(available_apis)}")
    
    # åŠ è½½é…ç½®
    config = load_config()
    
    # è·å–å½“å‰æ—¥æœŸ
    today = datetime.now().strftime("%Y-%m-%d")
    logger.info(f"åˆ†ææ—¥æœŸ: {today}")
    
    try:
        # åˆå§‹åŒ–ç»„ä»¶
        fetcher = ArXivFetcher(config)
        analyzer = LLMAnalyzer(config)
        processor = DataProcessor(config)
        
        # 1. æŠ“å–æœ€æ–°è®ºæ–‡
        logger.info("æ­£åœ¨æŠ“å–ArXivæœ€æ–°è®ºæ–‡...")
        papers = await fetcher.fetch_recent_papers()
        logger.info(f"æˆåŠŸæŠ“å– {len(papers)} ç¯‡è®ºæ–‡")
        
        if not papers:
            logger.warning("ä»Šæ—¥æ— æ–°è®ºæ–‡ï¼Œè·³è¿‡åˆ†æ")
            return
        
        # 2. ä¿å­˜åŸå§‹è®ºæ–‡æ•°æ®ï¼ˆå»é‡å¤„ç†ï¼‰
        await processor.save_papers(papers, today)
        
        # 3. æ£€æŸ¥å“ªäº›è®ºæ–‡éœ€è¦åˆ†æ
        papers_to_analyze = await processor.get_papers_needing_analysis(papers, today)
        
        if not papers_to_analyze:
            logger.info("æ‰€æœ‰è®ºæ–‡å·²åˆ†æå®Œæˆï¼Œæ— éœ€é‡å¤åˆ†æ")
        else:
            logger.info(f"éœ€è¦åˆ†æ {len(papers_to_analyze)} ç¯‡è®ºæ–‡")
            
            # 4. ä½¿ç”¨LLMåˆ†æè®ºæ–‡
            logger.info("å¼€å§‹LLMåˆ†æ...")
            analysis_results = []
            
            for i, paper in enumerate(papers_to_analyze, 1):
                logger.info(f"åˆ†æè®ºæ–‡ {i}/{len(papers_to_analyze)}: {paper['title'][:50]}...")
                
                try:
                    analysis = await analyzer.analyze_paper(paper)
                    analysis_results.append(analysis)
                except Exception as e:
                    logger.error(f"åˆ†æè®ºæ–‡å¤±è´¥: {e}")
                    # å³ä½¿å¤±è´¥ä¹Ÿè¦ä¿å­˜é”™è¯¯ä¿¡æ¯
                    analysis_results.append({
                        'timestamp': datetime.now().isoformat(),
                        'paper_id': paper.get('arxiv_id'),
                        'analysis': None,
                        'classification': None,
                        'keywords': [],
                        'summary': None,
                        'error': str(e)
                    })
            
            # 5. ä¿å­˜åˆ†æç»“æœï¼ˆå»é‡å¤„ç†ï¼‰
            await processor.save_analysis_results(analysis_results, today)
            
            logger.info(f"âœ… æœ¬æ¬¡åˆ†æå®Œæˆï¼å¤„ç†äº† {len(analysis_results)} ç¯‡è®ºæ–‡")
        
        # 6. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Šï¼ˆåŸºäºæ‰€æœ‰è®ºæ–‡ï¼‰
        all_analysis_results = await processor._load_existing_analysis(
            processor.base_data_dir / "analysis" / today
        )
        if all_analysis_results:
            await processor.generate_daily_summary(all_analysis_results, today)
            await processor.update_statistics(all_analysis_results, today)
            logger.info(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Šå·²æ›´æ–°ï¼Œæ€»è®¡ {len(all_analysis_results)} ç¯‡è®ºæ–‡")
        
    except Exception as e:
        logger.error(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise

def cli_main():
    """å‘½ä»¤è¡Œå…¥å£ç‚¹å‡½æ•°"""
    asyncio.run(main())

if __name__ == "__main__":
    cli_main()
